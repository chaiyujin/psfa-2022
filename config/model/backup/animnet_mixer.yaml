_target_: src.projects.speech_animation.pl_animnet_mixer.PL_AnimNetMixer
_init_: src.projects.speech_animation.pl_animnet_mixer.pre_compute_paddings

debug: ${debug}
video: ${data.video}
audio: ${data.audio}
wanna_fps: 30

# tag: ds${data.audio.sliding_window.deepspeech}_${.animnet.encoder.audio.using}-${cond:${.animnet.sequential.conv.causal},causal,conv}
tag: "test"

# * ---------------------------------------------------------------------------------------------------------------- * #
# *                                                    Visualizer                                                    * #
# * ---------------------------------------------------------------------------------------------------------------- * #

visualizer:
  painter: demo_mixer
  demo_mixer:
    label: ${model.using_label}
  # tb plot settings
  draw_gap_steps: 200
  draw_grid_size: 256
  # video settings
  video_gap_epochs: 10
  video_grid_size:  256
  video_1st_epoch:  false
  # generate or not
  generate_train: true
  generate_valid: true
  generate_test:  false
  # dump or not
  dump_metrics: true
  dump_audio:   false
  dump_offsets: false
  dump_images:  false
  dump_coeffs:  false
  # overwrite or not
  overwrite_videos: false

# * ---------------------------------------------------------------------------------------------------------------- * #
# *                                                  Neural Renderer                                                 * #
# * ---------------------------------------------------------------------------------------------------------------- * #

freeze_renderer: true
neural_renderer:
  load_from:
    path: ""
    possible_names: ["neural_renderer"]
    state_dict_remove_prefix: ["renderer."]
    freeze_loaded: ${...freeze_renderer}

# * ---------------------------------------------------------------------------------------------------------------- * #
# *                                                      AnimNet                                                     * #
# * ---------------------------------------------------------------------------------------------------------------- * #

using_label: "viseme"

freeze_animnet: false
animnet:
  # output deformation
  deform_from: "idle"
  # sequence length
  src_seq_frames: 20
  tgt_seq_frames: 20
  src_seq_pads: [0, 0]  # should be automaticly computed in pre_compute_paddings
  # audio
  company_audio: "win"  # win | one
  using_audio_feature: "deepspeech"

  n_visemes: 14

  # * ---------------------------------------------- per-frame encoder --------------------------------------------- * #

  encoder:

    audio:
      using: "xfmr"
      feature: "deepspeech"
      conv: { out_channels: 64, layer_norm: true, dropout: 0.1 }
      attn: { name: "bah", query_radius: 2, n_layers: 3, memory_size: 128, num_units: 64, smooth: false}
      xfmr: { d_model: 64, n_head: 4, n_enc_layers: 3, n_dec_layers: 1, dropout: 0.1, layer_norm: true }

    style_z:
        using: "embed"
        n_speakers: ${len:${dataset.style_ids}}
        embed: { embedding_size: 32, init_std: 0.1 }

    refxy_lower:
      using: m2i
      part: face_lower
      m2i: {mode: nearest, image_size: 64}
      nfc: 32
      out_channels: 128

    refxy_upper:
      using: m2i
      part: eyebrows
      m2i: {mode: nearest, image_size: 64}
      nfc: 8
      out_channels: 8

    ch_style_xy: 16
    ctt_method: 'gate'

  # * ---------------------------------------------- sequential module --------------------------------------------- * #

  sequential:
    using: "conv"
    conv:
      causal: true
      hidden_channels: [64, 128, 256]
      kernel_size: [5, 3, 3]
      dilation: 1
      norm_method: none
      dropout: 0.1

  # * ---------------------------------------------- per-frame decoder --------------------------------------------- * #

  decoder:
    verts:
      using: blendshape

      morphable:
        using: flame
        flame:
          n_shape: 100
          n_exp: 50

      blendshape:
        trainable: true
        using_basis: 'flame'
        n_components: 50
        flame_model_path: ${..morphable.flame.model_path}
        # mlp projection
        shared_mlp: { out_channels: [256, 256] }
        specific_mlp: { hidden_channels: [128] }
        norm_method: "none"
        activation: 'lrelu0.2'


# * ---------------------------------------------------------------------------------------------------------------- * #
# *                                                Optimizer settings                                                * #
# * ---------------------------------------------------------------------------------------------------------------- * #

optim_animnet:
  name: "Adam"
  lr: 1e-4
  weight_decay: 1e-8
  # lr_pca: 1e-6  # only work when decoder.delta.blendshape.trainable is true
  # lr_scheduler: { name: LambdaLR, interval: epoch, gamma: 0.98, start_iter: 50, min_times: 0.1 }

# * ---------------------------------------------------------------------------------------------------------------- * #
# *                                                       Loss                                                       * #
# * ---------------------------------------------------------------------------------------------------------------- * #

loss:
  # mesh
  mesh:
    pos: 1.0
    motion: 5.0
    non_face: 1e-3
  # label
  label:
    viseme_ce: 10.0
  # latent
  latent:
    using: 'l2'
    paired_close: 1000.0

  # scale of different loss
  scale:
    paired: 1.0
    mismatch: 2.0
    coma: 1.0
    random_flame: 0.0
